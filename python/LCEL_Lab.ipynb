{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64wN9geuXoFq"
   },
   "source": [
    "# Setup\n",
    "Set up your API keys (e.g., OpenAI API key) in a .env file or as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KbW8Y5-XwA2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env, doublecheck_pkgs\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your_api_key_here\"\n",
    "# Check and print results\n",
    "doublecheck_env(\".env\")  # check environmental variables\n",
    "doublecheck_pkgs(pyproject_path=\"pyproject.toml\", verbose=True)   # check packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72JWbICCX24S"
   },
   "source": [
    "# 1. Simple LLM Chain: Customer Inquiry Routing\n",
    "A common banking scenario is routing customer inquiries to the appropriate department (e.g., loans, credit cards, technical support). We'll use LCEL to build a simple classification chain.\n",
    "\n",
    "**Concept**: Chaining a prompt template and an LLM using the pipe | operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UelDyCHYGI5"
   },
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "# Define the banking-specific prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a customer service assistant at a bank. Classify the following customer inquiry into one of these categories: 'Loan Department', 'Credit Card Services', 'Online Banking Support', 'General Inquiry'. Only return the category name.\"),\n",
    "        (\"human\", \"{user_inquiry}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Build the LCEL chain\n",
    "# The output of prompt_template becomes the input to llm, and the output of llm becomes the input to output_parser\n",
    "classification_chain = prompt_template | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a banking-related inquiry\n",
    "inquiry_1 = \"I can't log in to my mobile app, it keeps showing an error code.\"\n",
    "response_1 = classification_chain.invoke({\"user_inquiry\": inquiry_1})\n",
    "print(f\"Inquiry: {inquiry_1}\")\n",
    "print(f\"Category: {response_1}\")\n",
    "\n",
    "inquiry_2 = \"What are the interest rates for a home loan?\"\n",
    "response_2 = classification_chain.invoke({\"user_inquiry\": inquiry_2})\n",
    "print(f\"\\nInquiry: {inquiry_2}\")\n",
    "print(f\"Category: {response_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPlhS2UQYOZD"
   },
   "source": [
    "# 2. Using `RunnableParallel` for Compliance Checks\n",
    "Banks often need to run multiple checks on a single piece of input, such as a customer's request. We can process these checks in parallel to save time.\n",
    "\n",
    "**Concept**: Use `RunnableParallel` to send an input to multiple independent chains concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BoMb7MfYbP2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Define specialized prompts for different banking checks\n",
    "fraud_check_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the transaction request: '{transaction_details}'. Is this likely fraudulent? Respond with 'Yes' or 'No' and a brief reason.\"\n",
    ")\n",
    "\n",
    "compliance_check_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the transaction request: '{transaction_details}'. Does it comply with anti-money laundering (AML) regulations? Respond with 'Compliant' or 'Non-Compliant' and a brief reason.\"\n",
    ")\n",
    "\n",
    "# Create the parallel chains\n",
    "# RunnablePassthrough allows the original input (transaction_details) to be passed to both branches\n",
    "parallel_checks = RunnableParallel(\n",
    "    fraud_status=fraud_check_prompt | llm | output_parser,\n",
    "    aml_status=compliance_check_prompt | llm | output_parser\n",
    ")\n",
    "\n",
    "# Invoke the parallel chain\n",
    "transaction_details = \"Transfer of $50,000 to an unverified international account with no prior transaction history.\"\n",
    "results = parallel_checks.invoke({\"transaction_details\": transaction_details})\n",
    "print(f\"Transaction Details: {transaction_details}\")\n",
    "print(f\"Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckI9-Ry2YiED"
   },
   "source": [
    "# 3. Using RunnableLambda for Data Pre-processing (Credit Scoring)\n",
    "Before a loan application is processed by an LLM or a model, it might need data cleaning or reformatting. RunnableLambda lets us insert custom Python functions into our LCEL chain.\n",
    "\n",
    "**Concept**: Wrapping a custom Python function to make it a compatible Runnable component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXjya5UHY4Vn"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A custom Python function to clean and format a credit report summary\n",
    "def format_credit_data(report_summary: str) -> dict:\n",
    "    # In a real scenario, this function would handle complex parsing/cleaning\n",
    "    print(f\"--- Processing raw data: {report_summary[:30]}...\")\n",
    "    # Simulate data formatting for the LLM\n",
    "    formatted_data = {\n",
    "        \"credit_score\": report_summary.split('Score: ')[1].split('.')[0],\n",
    "        \"late_payments\": \"Yes\" if \"late payments\" in report_summary.lower() else \"No\"\n",
    "    }\n",
    "    print(f\"--- Formatted data: {formatted_data}\")\n",
    "    return formatted_data\n",
    "\n",
    "# Define the prompt that takes the formatted dictionary as input\n",
    "credit_decision_prompt = ChatPromptTemplate.from_template(\n",
    "    \"A potential loan applicant has a credit score of {credit_score} and late payments status is {late_payments}. Provide a brief recommendation for a loan officer (Approve/Deny/Review).\"\n",
    ")\n",
    "\n",
    "# Build the chain: lambda function -> prompt -> llm -> parser\n",
    "# The lambda function runs first, and its output (a dictionary) is passed to the prompt\n",
    "credit_chain = RunnableLambda(format_credit_data) | credit_decision_prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain\n",
    "raw_report = \"Customer credit report: Score: 720. No recent late payments. Credit utilization is 30%.\"\n",
    "decision = credit_chain.invoke(raw_report)\n",
    "print(f\"\\nLoan Officer Recommendation: {decision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M97EJxOeY87u"
   },
   "source": [
    "# 4. Full RAG Chain: Financial Document Q&A\n",
    "This advanced example demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline for answering questions about a bank's internal policy documents, ensuring accuracy and compliance.\n",
    "\n",
    "**Concept**: Combining retrieval (data lookup) and generation (LLM response) into a single, seamless LCEL chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szV3dJbfZEQa"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#from pydantic import SecretStr\n",
    "\n",
    "# 1. Load a dummy banking policy document (replace with real PDF path)\n",
    "# A dummy text is used here for demonstration.\n",
    "# In a real case, load an actual PDF file, e.g., 'internal_banking_policy.pdf'\n",
    "# For now, we'll simulate the process with a simple string\n",
    "policy_text = \"\"\"\n",
    "Internal Banking Policy on Account Freezing (Effective 2025):\n",
    "Accounts may be frozen in cases of suspected fraud, anti-money laundering (AML) violations, or court orders.\n",
    "The compliance department is responsible for initiating freezes.\n",
    "Customers must be notified within 24 hours of a freeze via registered mail and email.\n",
    "To unfreeze an account, the customer must visit a branch with two forms of valid ID and proof of transaction legitimacy.\n",
    "\"\"\"\n",
    "\n",
    "# Simulate document creation and splitting\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=policy_text)]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# 2. Embed and store the document chunks (using in-memory Chroma for simplicity)\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(encode_kwargs={'normalize_embeddings': True},model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. Define the RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a bank policy expert. Use ONLY the provided internal policy context to answer the customer's question. If the answer is not in the context, state that the information is unavailable.\"),\n",
    "        (\"human\", \"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "# Invoke the RAG chain with a banking-related question\n",
    "question = \"What is the procedure for a customer to unfreeze their account?\"\n",
    "rag_response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_response}\")\n",
    "\n",
    "question_unrelated = \"What is the bank's stock price today?\"\n",
    "rag_response_unrelated = rag_chain.invoke(question_unrelated)\n",
    "print(f\"\\nQuestion: {question_unrelated}\")\n",
    "print(f\"Answer: {rag_response_unrelated}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6SnP4r+FMo5ECyOJ0zvIk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
